import streamlit as st
import pandas as pd
import requests
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from io import StringIO
import re
import time
import openpyxl

# -------------------------------
# í‚¤ì›Œë“œ ë° ì¹´í…Œê³ ë¦¬ ê¸°ì¤€ ì •ì˜
# -------------------------------
keywords = ["ì²œì¬êµìœ¡", "ì²œì¬êµê³¼ì„œ", "ì§€í•™ì‚¬", "ë²½í˜¸", "í”„ë¦°í”¼ì•„", "ë¯¸ë˜ì—”", "êµê³¼ì„œ", "ë™ì•„ì¶œíŒ"]
category_keywords = {
    "í›„ì›": ["í›„ì›", "ê¸°íƒ"],
    "ê¸°ë¶€": ["ê¸°ë¶€"],
    "í˜‘ì•½/MOU": ["í˜‘ì•½", "mou"],
    "ì—ë“€í…Œí¬/ë””ì§€í„¸êµìœ¡": ["ì—ë“€í…Œí¬", "ë””ì§€í„¸êµìœ¡", "aiêµìœ¡", "ìŠ¤ë§ˆíŠ¸êµìœ¡"],
    "ì •ì±…": ["ì •ì±…"],
    "ì¶œíŒ": ["ì¶œíŒ"],
    "ì¸ì‚¬/ì±„ìš©": ["ì±„ìš©", "êµì‚¬"],
    "í”„ë¦°íŠ¸ ë° ì¸ì‡„": ["ì¸ì‡„", "í”„ë¦°íŠ¸"],
    "ê³µê¸‰": ["ê³µê¸‰"],
    "êµìœ¡": ["êµìœ¡"],
    "ì´ë²¤íŠ¸": ["ì´ë²¤íŠ¸", "ì‚¬ì€í’ˆ"]
}

# ì¹´ì¹´ì˜¤í†¡ ê¸°ì¤€
kakao_categories = {
    "ì±„íƒ: ì„ ì • ê¸°ì¤€/í‰ê°€": ["í‰ê°€í‘œ", "ê¸°ì¤€", "ì¶”ì²œì˜ê²¬ì„œ", "ì„ ì •ê¸°ì¤€"],
    "ì±„íƒ: ìœ„ì›íšŒ ìš´ì˜": ["ìœ„ì›íšŒ", "í˜‘ì˜íšŒ", "ëŒ€í‘œêµì‚¬", "ìœ„ì›"],
    "ì±„íƒ: íšŒì˜/ì‹¬ì˜ ì§„í–‰": ["íšŒì˜", "íšŒì˜ë¡", "ì‹¬ì˜", "ì‹¬ì‚¬", "ìš´ì˜"],
    "ë°°ì†¡": ["ë°°ì†¡"],
    "ë°°ì†¡: ì§€ë„ì„œ/ì „ì‹œë³¸ ë„ì°©": ["ë„ì°©", "ì™”ì–´ìš”", "ì „ì‹œë³¸", "ì§€ë„ì„œ", "ë°•ìŠ¤"],
    "ë°°ì†¡: ë¼ë²¨/ì •ë¦¬ ì—…ë¬´": ["ë¼ë²¨", "ë¶„ë¥˜", "ì •ë¦¬", "ì „ì‹œ ì¤€ë¹„"],
    "ì£¼ë¬¸: ì‹œìŠ¤í…œ ì‚¬ìš©": ["ë‚˜ì´ìŠ¤", "ì—ë“€íŒŒì¸", "ë“±ë¡", "ì…ë ¥"],
    "ì£¼ë¬¸: ê³µë¬¸/ì •ì‚°": ["ê³µë¬¸", "ì •ì‚°", "ë§ˆê°ì¼", "ìš”ì²­"],
    "ì¶œíŒì‚¬: ìë£Œ ìˆ˜ë ¹/ì´ë²¤íŠ¸": ["ë³´ì¡°ìë£Œ", "ìë£Œ", "ê¸°í”„í‹°ì½˜", "ì´ë²¤íŠ¸"],
    "ì¶œíŒì‚¬: ìë£Œ íšŒìˆ˜/ìš”ì²­": ["íšŒìˆ˜", "ìš”ì²­", "êµì‚¬ìš©"]
}
publishers = ["ë¯¸ë˜ì—”", "ë¹„ìƒ", "ë™ì•„", "ì•„ì´ìŠ¤í¬ë¦¼", "ì²œì¬", "ì¢‹ì€ì±…", "ì§€í•™ì‚¬", "ëŒ€êµ", "ì´ë£¸", "ëª…ì§„", "ì²œì¬êµìœ¡"]
subjects = ["êµ­ì–´", "ìˆ˜í•™", "ì‚¬íšŒ", "ê³¼í•™", "ì˜ì–´", "ë„ë•", "ìŒì•…", "ë¯¸ìˆ ", "ì²´ìœ¡"]
complaint_keywords = ["ì•ˆ ì™”ì–´ìš”", "ì•„ì§", "ëŠ¦ê²Œ", "ì—†ì–´ìš”", "ì˜¤ë¥˜", "ë¬¸ì œ", "ì™œ", "í—·ê°ˆë ¤", "ë¶ˆí¸", "ì•ˆì˜´", "ì§€ì—°", "ì•ˆë³´ì—¬ìš”", "ëª» ë°›ì•˜", "í˜ë“¤ì–´ìš”"]

# -------------------------------
# ë‰´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
# -------------------------------
def crawl_news_bs(keyword, pages=5):
    headers = {"User-Agent": "Mozilla/5.0"}
    results = []
    seen = set()
    today = datetime.today().date()
    two_weeks_ago = today - timedelta(days=14)

    for page in range(1, pages + 1):
        start = (page - 1) * 10 + 1
        url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sort=1&nso=so%3Add%2Cp%3A2w&start={start}"
        try:
            res = requests.get(url, headers=headers)
            soup = BeautifulSoup(res.text, "lxml")
            articles = soup.select(".news_area")
            for a in articles:
                try:
                    title_elem = a.select_one(".news_tit")
                    title = title_elem.get("title")
                    link = title_elem.get("href")
                    summary = a.select_one(".dsc_txt_wrap").get_text(strip=True)
                    press = a.select_one(".info_group a").get_text(strip=True)
                    if link in seen or summary in seen:
                        continue
                    seen.add(link)
                    seen.add(summary)

                    body = get_news_body(link)
                    full_text = (summary + " " + body).lower()

                    date_str = get_news_date(link)
                    try:
                        article_date = datetime.strptime(date_str, "%Y.%m.%d").date()
                        if article_date < two_weeks_ago:
                            continue
                    except:
                        continue

                    results.append({
                        "ì¶œíŒì‚¬ëª…": check_publisher(full_text),
                        "ì¹´í…Œê³ ë¦¬": categorize_news(full_text),
                        "ë‚ ì§œ": date_str,
                        "ì œëª©": title,
                        "URL": link,
                        "ìš”ì•½": summary,
                        "ì–¸ë¡ ì‚¬": press,
                        "ë‚´ìš©ì ê²€": match_keyword_flag(full_text),
                        "ë³¸ë¬¸ë‚´_êµê³¼ì„œ_ë˜ëŠ”_ë°œí–‰ì‚¬_ì–¸ê¸‰": contains_textbook(body)
                    })
                except:
                    continue
            time.sleep(0.3)
        except:
            continue
    return pd.DataFrame(results)

def get_news_body(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        soup = BeautifulSoup(res.text, 'lxml')
        for cls in ["article", "article-body", "newsEndContents", "content", "viewContent"]:
            tag = soup.find(class_=cls)
            if tag:
                return tag.get_text(" ", strip=True)
        return soup.get_text(" ", strip=True)
    except:
        return ""

def get_news_date(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        soup = BeautifulSoup(res.text, 'lxml')
        meta = soup.find("meta", {"property": "article:published_time"})
        if meta and meta.get("content"):
            return meta["content"][:10].replace("-", ".")
        return "ë‚ ì§œ ì—†ìŒ"
    except:
        return "ë‚ ì§œ ì˜¤ë¥˜"

def categorize_news(text):
    text = text.lower()
    for category, words in category_keywords.items():
        if any(w in text for w in words):
            return category
    return "ê¸°íƒ€"

def check_publisher(text):
    for pub in keywords:
        if pub.lower() in text:
            return pub
    return "ê¸°íƒ€"

def match_keyword_flag(text):
    for pub in keywords:
        if pub.lower() in text:
            return "O"
    return "X"

def contains_textbook(text):
    return "O" if "êµê³¼ì„œ" in text or "ë°œí–‰ì‚¬" in text else "X"

# -------------------------------
# ì¹´ì¹´ì˜¤í†¡ ë¶„ì„ í•¨ìˆ˜
# -------------------------------
def analyze_kakao(text):
    pattern = re.compile(r"(?P<datetime>\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼ (ì˜¤ì „|ì˜¤í›„) \d{1,2}:\d{2}), (?P<sender>[^:]+) : (?P<message>.+)")
    matches = pattern.findall(text)
    rows = []
    for match in matches:
        date_str, ampm, sender, message = match
        if sender.strip() == "ì˜¤í”ˆì±„íŒ…ë´‡":
            continue
        try:
            dt = datetime.strptime(date_str.replace("ì˜¤ì „", "AM").replace("ì˜¤í›„", "PM"), "%Yë…„ %mì›” %dì¼ %p %I:%M")
            rows.append({
                "ë‚ ì§œ": dt.date(),
                "ì‹œê°„": dt.time(),
                "ë³´ë‚¸ ì‚¬ëŒ": sender.strip(),
                "ë©”ì‹œì§€": message.strip(),
                "ì¹´í…Œê³ ë¦¬": classify_category(message),
                "ì¶œíŒì‚¬": extract_kakao_publisher(message),
                "ê³¼ëª©": extract_subject(message),
                "ë¶ˆë§Œ ì—¬ë¶€": detect_complaint(message)
            })
        except:
            continue
    return pd.DataFrame(rows)

def classify_category(text):
    if "ë°°ì†¡" in text:
        return "ë°°ì†¡"
    for category, words in kakao_categories.items():
        if any(w in text for w in words):
            return category
    return "ê¸°íƒ€"

def extract_kakao_publisher(text):
    for pub in publishers:
        if pub in text:
            return pub
    return None

def extract_subject(text):
    for subject in subjects:
        if subject in text:
            return subject
    return None

def detect_complaint(text):
    return any(w in text for w in complaint_keywords)

# -------------------------------
# Streamlit UI
# -------------------------------
st.set_page_config(page_title="ì˜¬ì¸ì› êµê³¼ì„œ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“š êµê³¼ì„œ ì»¤ë®¤ë‹ˆí‹° ë¶„ì„ & ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¬ì¸ì› ì•±")

tab1, tab2 = st.tabs(["ğŸ’¬ ì¹´ì¹´ì˜¤í†¡ ë¶„ì„", "ğŸ“° ë‰´ìŠ¤ í¬ë¡¤ë§"])

with tab1:
    st.subheader("ğŸ“‚ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë¶„ì„ê¸°")
    uploaded_file = st.file_uploader("ì¹´ì¹´ì˜¤í†¡ .txt íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="txt")
    if uploaded_file:
        stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
        df_kakao = analyze_kakao(stringio.read())
        st.success("âœ… ëŒ€í™” ë¶„ì„ ì™„ë£Œ")
        st.dataframe(df_kakao)
        st.download_button("ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", data=df_kakao.to_csv(index=False).encode("utf-8"), file_name="ì¹´ì¹´ì˜¤í†¡_ë¶„ì„ê²°ê³¼.csv", mime="text/csv")

with tab2:
    st.subheader("ğŸ“° ì¶œíŒì‚¬ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° (ìµœê·¼ 2ì£¼)")
    st.markdown("ğŸ” **ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”**")
    selected_keywords = st.multiselect("â€» ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥", options=keywords, default=keywords)

    if not selected_keywords:
        st.warning("âš ï¸ ìµœì†Œ í•˜ë‚˜ ì´ìƒì˜ í‚¤ì›Œë“œë¥¼ ì„ íƒí•´ ì£¼ì„¸ìš”.")
    elif st.button("í¬ë¡¤ë§ ì‹œì‘"):
        progress_bar = st.progress(0, text="ğŸ”„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤€ë¹„ ì¤‘...")
        status_placeholder = st.empty()

        total_tasks = len(selected_keywords)
        results = []
        start_time = time.time()

        for i, kw in enumerate(selected_keywords):
            kw_start = time.time()
            df = crawl_news_bs(kw, pages=5)
            results.append(df)

            # ì‹œê°„ ê³„ì‚°
            elapsed_time = time.time() - start_time
            avg_time_per_keyword = elapsed_time / (i + 1)
            remaining = int(avg_time_per_keyword * (total_tasks - (i + 1)))

            mins, secs = divmod(remaining, 60)
            eta = f"â± ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: ì•½ {mins}ë¶„ {secs}ì´ˆ"

            progress_pct = int(((i + 1) / total_tasks) * 100)
            progress_bar.progress(progress_pct, text=f"{kw} ì™„ë£Œ ({progress_pct}%)")
            status_placeholder.info(eta)

        df_news = pd.concat(results, ignore_index=True)
        status_placeholder.success("âœ… ì „ì²´ í¬ë¡¤ë§ ì™„ë£Œ!")

        st.success(f"ì´ {len(df_news)}ê±´ì˜ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ âœ…")
        st.dataframe(df_news)
        st.download_button(
            "ğŸ“¥ ë‰´ìŠ¤ ë°ì´í„° ë‹¤ìš´ë¡œë“œ",
            data=df_news.to_csv(index=False).encode("utf-8"),
            file_name="ì¶œíŒì‚¬_ë‰´ìŠ¤_í¬ë¡¤ë§_ê²°ê³¼.csv",
            mime="text/csv"
        )
