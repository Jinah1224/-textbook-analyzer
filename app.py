import streamlit as st
import pandas as pd
import requests
from datetime import datetime
from bs4 import BeautifulSoup
from io import StringIO
import re
import time
import openpyxl

# -------------------------------
# ë‰´ìŠ¤ í‚¤ì›Œë“œ ë° ì¹´í…Œê³ ë¦¬ ê¸°ì¤€ ì •ì˜
# -------------------------------
keywords = ["ì²œì¬êµìœ¡", "ì²œì¬êµê³¼ì„œ", "ì§€í•™ì‚¬", "ë²½í˜¸", "í”„ë¦°í”¼ì•„", "ë¯¸ë˜ì—”", "êµê³¼ì„œ", "ë™ì•„ì¶œíŒ"]
category_keywords = {
    "í›„ì›": ["í›„ì›", "ê¸°íƒ"],
    "ê¸°ë¶€": ["ê¸°ë¶€"],
    "í˜‘ì•½/MOU": ["í˜‘ì•½", "mou"],
    "ì—ë“€í…Œí¬/ë””ì§€í„¸êµìœ¡": ["ì—ë“€í…Œí¬", "ë””ì§€í„¸êµìœ¡", "ë””ì§€í„¸ êµìœ¡", "aiêµìœ¡", "ai êµìœ¡", "ìŠ¤ë§ˆíŠ¸êµìœ¡", "ìŠ¤ë§ˆíŠ¸ êµìœ¡"],
    "ì •ì±…": ["ì •ì±…"],
    "ì¶œíŒ": ["ì¶œíŒ"],
    "ì¸ì‚¬/ì±„ìš©": ["ì±„ìš©", "êµì‚¬"],
    "í”„ë¦°íŠ¸ ë° ì¸ì‡„": ["ì¸ì‡„", "í”„ë¦°íŠ¸"],
    "ê³µê¸‰": ["ê³µê¸‰"],
    "êµìœ¡": ["êµìœ¡"],
    "ì´ë²¤íŠ¸": ["ì´ë²¤íŠ¸", "ì‚¬ì€í’ˆ"]
}

# -------------------------------
# ì¹´ì¹´ì˜¤í†¡ ë¶„ì„ ê¸°ì¤€ ì •ì˜
# -------------------------------
kakao_categories = {
    "ì±„íƒ: ì„ ì • ê¸°ì¤€/í‰ê°€": ["í‰ê°€í‘œ", "ê¸°ì¤€", "ì¶”ì²œì˜ê²¬ì„œ", "ì„ ì •ê¸°ì¤€"],
    "ì±„íƒ: ìœ„ì›íšŒ ìš´ì˜": ["ìœ„ì›íšŒ", "í˜‘ì˜íšŒ", "ëŒ€í‘œêµì‚¬", "ìœ„ì›"],
    "ì±„íƒ: íšŒì˜/ì‹¬ì˜ ì§„í–‰": ["íšŒì˜", "íšŒì˜ë¡", "ì‹¬ì˜", "ì‹¬ì‚¬", "ìš´ì˜"],
    "ë°°ì†¡": ["ë°°ì†¡"],
    "ë°°ì†¡: ì§€ë„ì„œ/ì „ì‹œë³¸ ë„ì°©": ["ë„ì°©", "ì™”ì–´ìš”", "ì „ì‹œë³¸", "ì§€ë„ì„œ", "ë°•ìŠ¤"],
    "ë°°ì†¡: ë¼ë²¨/ì •ë¦¬ ì—…ë¬´": ["ë¼ë²¨", "ë¶„ë¥˜", "ì •ë¦¬", "ì „ì‹œ ì¤€ë¹„"],
    "ì£¼ë¬¸: ì‹œìŠ¤í…œ ì‚¬ìš©": ["ë‚˜ì´ìŠ¤", "ì—ë“€íŒŒì¸", "ë“±ë¡", "ì…ë ¥"],
    "ì£¼ë¬¸: ê³µë¬¸/ì •ì‚°": ["ê³µë¬¸", "ì •ì‚°", "ë§ˆê°ì¼", "ìš”ì²­"],
    "ì¶œíŒì‚¬: ìë£Œ ìˆ˜ë ¹/ì´ë²¤íŠ¸": ["ë³´ì¡°ìë£Œ", "ìë£Œ", "ê¸°í”„í‹°ì½˜", "ì´ë²¤íŠ¸"],
    "ì¶œíŒì‚¬: ìë£Œ íšŒìˆ˜/ìš”ì²­": ["íšŒìˆ˜", "ìš”ì²­", "êµì‚¬ìš©"]
}
publishers = ["ë¯¸ë˜ì—”", "ë¹„ìƒ", "ë™ì•„", "ì•„ì´ìŠ¤í¬ë¦¼", "ì²œì¬", "ì¢‹ì€ì±…", "ì§€í•™ì‚¬", "ëŒ€êµ", "ì´ë£¸", "ëª…ì§„", "ì²œì¬êµìœ¡"]
subjects = ["êµ­ì–´", "ìˆ˜í•™", "ì‚¬íšŒ", "ê³¼í•™", "ì˜ì–´", "ë„ë•", "ìŒì•…", "ë¯¸ìˆ ", "ì²´ìœ¡"]
complaint_keywords = ["ì•ˆ ì™”ì–´ìš”", "ì•„ì§", "ëŠ¦ê²Œ", "ì—†ì–´ìš”", "ì˜¤ë¥˜", "ë¬¸ì œ", "ì™œ", "í—·ê°ˆë ¤", "ë¶ˆí¸", "ì•ˆì˜´", "ì§€ì—°", "ì•ˆë³´ì—¬ìš”", "ëª» ë°›ì•˜", "í˜ë“¤ì–´ìš”"]

# -------------------------------
# ë‰´ìŠ¤ í¬ë¡¤ë§ ê´€ë ¨ í•¨ìˆ˜
# -------------------------------
def categorize_news(text):
    text = text.lower()
    for category, keywords in category_keywords.items():
        if any(k in text for k in keywords):
            return category
    return "ê¸°íƒ€"

def check_publisher(text):
    for pub in keywords:
        if pub.lower() in text:
            return pub
    return "ê¸°íƒ€"

def match_keyword_flag(text):
    for pub in keywords:
        if pub.lower() in text:
            return "O"
    return "X"

def contains_textbook(text):
    return "O" if "êµê³¼ì„œ" in text or "ë°œí–‰ì‚¬" in text else "X"

def get_news_body(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        if res.status_code != 200:
            return ""
        soup = BeautifulSoup(res.text, 'lxml')
        candidates = ["article", "article-body", "newsEndContents", "content", "viewContent"]
        for cls in candidates:
            tag = soup.find(class_=cls)
            if tag:
                return tag.get_text(" ", strip=True)
        return soup.get_text(" ", strip=True)
    except:
        return ""

def get_news_date(url):
    try:
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        soup = BeautifulSoup(res.text, 'lxml')
        meta = soup.find("meta", {"property": "article:published_time"})
        if meta and meta.get("content"):
            return meta["content"][:10].replace("-", ".")
        return "ë‚ ì§œ ì—†ìŒ"
    except:
        return "ë‚ ì§œ ì˜¤ë¥˜"

def crawl_news_bs(keyword, pages=10):
    headers = {"User-Agent": "Mozilla/5.0"}
    results = []
    seen_links = set()
    seen_summaries = set()

    for page in range(1, pages + 1):
        start = (page - 1) * 10 + 1
        search_url = f"https://search.naver.com/search.naver?where=news&query={keyword}&sort=1&nso=so%3Add%2Cp%3A2w&start={start}"
        res = requests.get(search_url, headers=headers)
        if res.status_code != 200:
            continue
        soup = BeautifulSoup(res.text, "lxml")
        articles = soup.select(".news_area")

        for a in articles:
            try:
                title_elem = a.select_one(".news_tit")
                title = title_elem.get("title")
                link = title_elem.get("href")
                summary_elem = a.select_one(".dsc_txt_wrap")
                summary = summary_elem.get_text(strip=True) if summary_elem else ""
                press = a.select_one(".info_group a").get_text(strip=True)

                if link in seen_links or summary in seen_summaries:
                    continue
                seen_links.add(link)
                seen_summaries.add(summary)

                body = get_news_body(link)
                full_text = (summary + " " + body).lower()

                results.append({
                    "ì¶œíŒì‚¬ëª…": check_publisher(full_text),
                    "ì¹´í…Œê³ ë¦¬": categorize_news(full_text),
                    "ë‚ ì§œ": get_news_date(link),
                    "ì œëª©": title,
                    "URL": link,
                    "ìš”ì•½": summary,
                    "ì–¸ë¡ ì‚¬": press,
                    "ë‚´ìš©ì ê²€": match_keyword_flag(full_text),
                    "ë³¸ë¬¸ë‚´_êµê³¼ì„œ_ë˜ëŠ”_ë°œí–‰ì‚¬_ì–¸ê¸‰": contains_textbook(body)
                })
            except:
                continue
        time.sleep(0.3)

    return pd.DataFrame(results)

# -------------------------------
# ì¹´ì¹´ì˜¤í†¡ ë¶„ì„ í•¨ìˆ˜
# -------------------------------
def analyze_kakao(text):
    pattern = re.compile(r"(?P<datetime>\d{4}ë…„ \d{1,2}ì›” \d{1,2}ì¼ (ì˜¤ì „|ì˜¤í›„) \d{1,2}:\d{2}), (?P<sender>[^:]+) : (?P<message>.+)")
    matches = pattern.findall(text)
    rows = []
    for match in matches:
        date_str, ampm, sender, message = match
        if sender.strip() == "ì˜¤í”ˆì±„íŒ…ë´‡":
            continue
        try:
            dt = datetime.strptime(date_str.replace("ì˜¤ì „", "AM").replace("ì˜¤í›„", "PM"), "%Yë…„ %mì›” %dì¼ %p %I:%M")
            rows.append({
                "ë‚ ì§œ": dt.date(),
                "ì‹œê°„": dt.time(),
                "ë³´ë‚¸ ì‚¬ëŒ": sender.strip(),
                "ë©”ì‹œì§€": message.strip(),
                "ì¹´í…Œê³ ë¦¬": classify_category(message),
                "ì¶œíŒì‚¬": extract_kakao_publisher(message),
                "ê³¼ëª©": extract_subject(message),
                "ë¶ˆë§Œ ì—¬ë¶€": detect_complaint(message)
            })
        except:
            continue
    return pd.DataFrame(rows)

def classify_category(text):
    if "ë°°ì†¡" in text:
        return "ë°°ì†¡"
    for category, keywords in kakao_categories.items():
        if any(k in text for k in keywords):
            return category
    return "ê¸°íƒ€"

def extract_kakao_publisher(text):
    for pub in publishers:
        if pub in text:
            return pub
    return None

def extract_subject(text):
    for subject in subjects:
        if subject in text:
            return subject
    return None

def detect_complaint(text):
    return any(k in text for k in complaint_keywords)

# -------------------------------
# Streamlit ì•± UI
# -------------------------------
st.set_page_config(page_title="ì˜¬ì¸ì› êµê³¼ì„œ ë¶„ì„ê¸°", layout="wide")
st.title("ğŸ“š êµê³¼ì„œ ì»¤ë®¤ë‹ˆí‹° ë¶„ì„ & ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¬ì¸ì› ì•±")

tab1, tab2 = st.tabs(["ğŸ’¬ ì¹´ì¹´ì˜¤í†¡ ë¶„ì„", "ğŸ“° ë‰´ìŠ¤ í¬ë¡¤ë§"])

with tab1:
    st.subheader("ğŸ“‚ ì¹´ì¹´ì˜¤í†¡ ëŒ€í™” ë¶„ì„ê¸°")
    uploaded_file = st.file_uploader("ì¹´ì¹´ì˜¤í†¡ .txt íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type="txt")
    if uploaded_file:
        stringio = StringIO(uploaded_file.getvalue().decode("utf-8"))
        df_kakao = analyze_kakao(stringio.read())
        st.success("âœ… ëŒ€í™” ë¶„ì„ ì™„ë£Œ")
        st.dataframe(df_kakao)

        st.download_button(
            "ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ",
            data=df_kakao.to_csv(index=False).encode("utf-8"),
            file_name="ì¹´ì¹´ì˜¤í†¡_ë¶„ì„ê²°ê³¼.csv",
            mime="text/csv"
        )

with tab2:
    st.subheader("ğŸ“° ì¶œíŒì‚¬ ê´€ë ¨ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° (ìµœê·¼ 2ì£¼)")
    if st.button("í¬ë¡¤ë§ ì‹œì‘"):
        with st.spinner("ğŸ” ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤..."):
            df_news = pd.concat([crawl_news_bs(kw) for kw in keywords], ignore_index=True)

        st.success(f"âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ - ì´ {len(df_news)}ê±´ ìˆ˜ì§‘ë¨")
        st.dataframe(df_news)

        st.download_button(
            "ğŸ“¥ ë‰´ìŠ¤ ë°ì´í„° ë‹¤ìš´ë¡œë“œ",
            data=df_news.to_csv(index=False).encode("utf-8"),
            file_name="ì¶œíŒì‚¬_ë‰´ìŠ¤_í¬ë¡¤ë§_ê²°ê³¼.csv",
            mime="text/csv"
        )
